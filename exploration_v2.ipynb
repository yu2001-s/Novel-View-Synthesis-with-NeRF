{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['results', 'tensorboard', 'src', 'temp_data', 'README.md', 'models', '.gitignore', 'wandb', 'exploration.ipynb', '.git', 'playground.ipynb', 'data', '.vscode', 'exploration_v2.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "P_PATH = os.getcwd()\n",
    "print(os.listdir(P_PATH))\n",
    "\n",
    "sys.path.append(P_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size:  16000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shao-yu-huang/anaconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from src.data_loader import *\n",
    "from src.utils import *\n",
    "from src.model import *\n",
    "\n",
    "SCALEDOWN = 2\n",
    "OBJ_NAME = 'chair'\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "img_size = int(800/SCALEDOWN)\n",
    "\n",
    "\n",
    "min_max = None\n",
    "\n",
    "#train dataset\n",
    "train_dataset = SynDatasetRay(obj_name=OBJ_NAME, root_dir=P_PATH, split=\"train\", img_size=img_size, num_points=8)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "min_max = train_dataset.min_max\n",
    "\n",
    "print(\"train dataset size: \", len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rays_o torch.Size([3])\n",
      "rays_d torch.Size([3])\n",
      "points torch.Size([8, 3])\n",
      "z_vals torch.Size([8, 1])\n",
      "v_dir torch.Size([2])\n",
      "rgb torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "#print the sample data shape\n",
    "sample = train_dataset[0]\n",
    "\n",
    "#sample is a dict with keys: rays_o, rays_d, points, z_vals, v_dir, img\n",
    "for key in sample:\n",
    "    print(key, sample[key].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 8, 60]) torch.Size([1024, 16])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "sample = next(iter(train_dataloader))\n",
    "\n",
    "# for key in sample:\n",
    "#     print(key, sample[key].shape)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "points = sample['points']\n",
    "v_dir = sample['v_dir']\n",
    "\n",
    "points_endcoded, v_dir_endcoded = position_encoding(points, v_dir, L_p=10, L_v=4)\n",
    "print(points_endcoded.shape, v_dir_endcoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model and volume rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rays_o torch.Size([1024, 3])\n",
      "rays_d torch.Size([1024, 3])\n",
      "points torch.Size([1024, 8, 3])\n",
      "z_vals torch.Size([1024, 8, 1])\n",
      "v_dir torch.Size([1024, 2])\n",
      "rgb torch.Size([1024, 3])\n",
      "\n",
      "torch.Size([1024, 8, 3]) torch.Size([1024, 8])\n",
      "\n",
      "torch.Size([1024, 3])\n",
      "torch.Size([1024, 3])\n",
      "tensor(0.0064, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#test model\n",
    "model = NeRF().to(device)\n",
    "model.eval()\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "sample = next(iter(train_dataloader))\n",
    "\n",
    "for key in sample:\n",
    "    print(key, sample[key].shape)\n",
    "print()\n",
    "\n",
    "points = sample['points'].to(device)\n",
    "v_dir = sample['v_dir'].to(device)\n",
    "z_vals = sample['z_vals'].to(device).squeeze()\n",
    "\n",
    "with torch.no_grad():\n",
    "    rgb, sigma = model(points, v_dir)\n",
    "    print(rgb.shape, sigma.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "#test volume rendering\n",
    "rendered_rgb = volume_rendering(z_vals, rgb, sigma)\n",
    "\n",
    "print(rendered_rgb.shape)\n",
    "print(sample['rgb'].shape)\n",
    "\n",
    "#calculate loss\n",
    "loss = torch.nn.functional.mse_loss(rendered_rgb, sample['rgb'].to(device))\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([104, 8, 3])\n",
      "sigma shape:  torch.Size([104, 8])\n",
      "z_vals shape:  torch.Size([104, 8])\n",
      "val loss:  0.2096224781125784\n",
      "train loss:  0.20619143545627594\n",
      "epoch:  1\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([104, 8, 3])\n",
      "sigma shape:  torch.Size([104, 8])\n",
      "z_vals shape:  torch.Size([104, 8])\n",
      "val loss:  0.1352740339934826\n",
      "train loss:  0.13424968719482422\n",
      "epoch:  2\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([104, 8, 3])\n",
      "sigma shape:  torch.Size([104, 8])\n",
      "z_vals shape:  torch.Size([104, 8])\n",
      "val loss:  0.0754540846683085\n",
      "train loss:  0.07626868784427643\n",
      "epoch:  3\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([104, 8, 3])\n",
      "sigma shape:  torch.Size([104, 8])\n",
      "z_vals shape:  torch.Size([104, 8])\n",
      "val loss:  0.07695594895631075\n",
      "train loss:  0.10482867807149887\n",
      "epoch:  4\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([104, 8, 3])\n",
      "sigma shape:  torch.Size([104, 8])\n",
      "z_vals shape:  torch.Size([104, 8])\n",
      "val loss:  0.07663274183869362\n",
      "train loss:  0.09096449613571167\n",
      "epoch:  5\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([104, 8, 3])\n",
      "sigma shape:  torch.Size([104, 8])\n",
      "z_vals shape:  torch.Size([104, 8])\n",
      "val loss:  0.07648358074948192\n",
      "train loss:  0.08459926396608353\n",
      "epoch:  6\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([104, 8, 3])\n",
      "sigma shape:  torch.Size([104, 8])\n",
      "z_vals shape:  torch.Size([104, 8])\n",
      "val loss:  0.07643956178799272\n",
      "train loss:  0.08272121846675873\n",
      "epoch:  7\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([104, 8, 3])\n",
      "sigma shape:  torch.Size([104, 8])\n",
      "z_vals shape:  torch.Size([104, 8])\n",
      "val loss:  0.07694399636238813\n",
      "train loss:  0.10424377024173737\n",
      "epoch:  8\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([104, 8, 3])\n",
      "sigma shape:  torch.Size([104, 8])\n",
      "z_vals shape:  torch.Size([104, 8])\n",
      "val loss:  0.07583163259550929\n",
      "train loss:  0.05678274855017662\n",
      "epoch:  9\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([128, 8, 3])\n",
      "sigma shape:  torch.Size([128, 8])\n",
      "z_vals shape:  torch.Size([128, 8])\n",
      "rgb shape:  torch.Size([104, 8, 3])\n",
      "sigma shape:  torch.Size([104, 8])\n",
      "z_vals shape:  torch.Size([104, 8])\n",
      "val loss:  0.07684510201215744\n",
      "train loss:  0.10002418607473373\n"
     ]
    }
   ],
   "source": [
    "#run a small training loop\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "model = NeRF(D=6, W=128, skips=[3]).to(device)\n",
    "model.train()\n",
    "BATCH_SIZE = 128\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "indice = torch.randint(0, 160000, (1000,))\n",
    "v_indice = torch.randint(0, 160000, (1000,))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=SubsetRandomSampler(indice), num_workers=NUM_WORKERS)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=SubsetRandomSampler(v_indice), num_workers=NUM_WORKERS)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     print(\"epoch: \", epoch)\n",
    "#     for i, sample in enumerate(train_dataloader):\n",
    "#         points = sample['points'].to(device)\n",
    "#         v_dir = sample['v_dir'].to(device)\n",
    "#         z_vals = sample['z_vals'].to(device).squeeze()\n",
    "#         rgb_gt = sample['rgb'].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         rgb, sigma = model(points, v_dir)\n",
    "#         rgb_pred = volume_rendering(z_vals, rgb, sigma)\n",
    "#         loss = torch.nn.functional.mse_loss(rgb_pred, rgb_gt)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         #validation loss\n",
    "#         if i % 10 == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 val_loss_total = 0\n",
    "#                 for j, sample in enumerate(val_dataloader):\n",
    "#                     points = sample['points'].to(device)\n",
    "#                     v_dir = sample['v_dir'].to(device)\n",
    "#                     z_vals = sample['z_vals'].to(device).squeeze()\n",
    "#                     rgb_gt = sample['rgb'].to(device)\n",
    "\n",
    "#                     rgb, sigma = model(points, v_dir)\n",
    "#                     print(\"rgb shape: \", rgb.shape)\n",
    "#                     print(\"sigma shape: \", sigma.shape)\n",
    "#                     print(\"z_vals shape: \", z_vals.shape)\n",
    "#                     rgb_pred = volume_rendering(z_vals, rgb, sigma)\n",
    "#                     loss = torch.nn.functional.mse_loss(rgb_pred, rgb_gt)\n",
    "#                     val_loss_total += loss.item()\n",
    "\n",
    "#                 print(\"val loss: \", val_loss_total / len(val_dataloader))\n",
    "#                 print(\"train loss: \", loss.item())\n",
    "                    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test trainer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shao-yu-huang/Desktop/ml-workspace/Novel-View-Synthesis-with-NeRF/wandb/run-20231210_121039-vjbnbw8a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peter90519051/nerf/runs/vjbnbw8a' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/peter90519051/nerf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peter90519051/nerf' target=\"_blank\">https://wandb.ai/peter90519051/nerf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peter90519051/nerf/runs/vjbnbw8a' target=\"_blank\">https://wandb.ai/peter90519051/nerf/runs/vjbnbw8a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Loss: 0.2133: : 8it [00:00, 34.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 0.1881 | Val Loss: 0.1995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 | Loss: 0.0819: : 8it [00:00, 34.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 | Train Loss: 0.1552 | Val Loss: 0.0737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 | Loss: 0.0490: : 8it [00:00, 35.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 | Train Loss: 0.0677 | Val Loss: 0.0864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 | Loss: 0.0577: : 8it [00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 | Train Loss: 0.0709 | Val Loss: 0.0863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 | Loss: 0.0273: : 8it [00:00, 36.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 | Train Loss: 0.0702 | Val Loss: 0.0864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 | Loss: 0.0619: : 8it [00:00, 29.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 | Train Loss: 0.0710 | Val Loss: 0.0858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 | Loss: 0.0902: : 8it [00:00, 31.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 | Train Loss: 0.0716 | Val Loss: 0.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 | Loss: 0.1010: : 8it [00:00, 31.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 | Train Loss: 0.0719 | Val Loss: 0.0861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 | Loss: 0.0839: : 8it [00:00, 30.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 | Train Loss: 0.0715 | Val Loss: 0.0857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 | Loss: 0.0451: : 8it [00:00, 32.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 | Train Loss: 0.0706 | Val Loss: 0.0859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 | Loss: 0.0840: : 8it [00:00, 32.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 | Train Loss: 0.0715 | Val Loss: 0.0869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 | Loss: 0.0525: : 8it [00:00, 30.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▆▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▁▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>11</td></tr><tr><td>train_loss</td><td>0.07076</td></tr><tr><td>val_loss</td><td>0.08714</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test</strong> at: <a href='https://wandb.ai/peter90519051/nerf/runs/vjbnbw8a' target=\"_blank\">https://wandb.ai/peter90519051/nerf/runs/vjbnbw8a</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231210_121039-vjbnbw8a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from src.trainer import *\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "\n",
    "#init model\n",
    "D = 6\n",
    "W = 128\n",
    "input_ch_pos = 3\n",
    "input_ch_dir = 2\n",
    "L_p = 10\n",
    "L_v = 4\n",
    "skips = [3]\n",
    "\n",
    "lr = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "wandb.init(project=\"nerf\", \n",
    "           name=\"test\",\n",
    "           config={\n",
    "                \"D\": D,\n",
    "                \"W\": W,\n",
    "                \"input_ch_pos\": input_ch_pos,\n",
    "                \"input_ch_dir\": input_ch_dir,\n",
    "                \"L_p\": L_p,\n",
    "                \"L_v\": L_v,\n",
    "                \"skips\": skips,\n",
    "                \"lr\": lr,\n",
    "                \"BATCH_SIZE\": BATCH_SIZE\n",
    "              }  \n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "model = NeRF(D=D, W=W, input_ch_pos=input_ch_pos, input_ch_dir=input_ch_dir, L_p=L_p, L_v=L_v, skips=skips).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "#init optimizer\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "#create a subset of the train dataset\n",
    "train_i = torch.randint(0, 160000, (1000,))\n",
    "val_i = torch.randint(0, 160000, (1000,))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,sampler=SubsetRandomSampler(train_i), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "val_dataloader = DataLoader(train_dataset,sampler=SubsetRandomSampler(val_i) , batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "#init trainer\n",
    "trainer = NeRFTrainer(model=model, optimizer=optimizer, \n",
    "                      lr_scheduler=lr_scheduler, loss_fn=loss_fn, \n",
    "                      train_loader=train_dataloader, val_loader=val_dataloader, \n",
    "                      device=device, wandb_run=True)\n",
    "\n",
    "trainer.train(epochs=100, log_interval=1, early_stopping_patience=10)\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
