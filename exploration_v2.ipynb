{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shao-yu-huang/Desktop/ml-workspace/Novel-View-Synthesis-with-NeRF\n",
      "['train.py', 'results', 'src', 'README.md', 'models', '.gitignore', 'wandb', 'exploration.ipynb', '.git', 'playground.ipynb', 'data', '.vscode', 'exploration_v2.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "P_PATH = os.getcwd()\n",
    "print(P_PATH)\n",
    "print(os.listdir(P_PATH))\n",
    "\n",
    "sys.path.append(P_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:424] . unexpected pos 85122368 vs 85122320",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:853\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    852\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 853\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/5: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m img_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m800\u001b[39m\u001b[38;5;241m/\u001b[39mSCALEDOWN)\n\u001b[1;32m     17\u001b[0m min_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mdata_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOBJ_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#train dataset\u001b[39;00m\n\u001b[1;32m     22\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SynDatasetRay(obj_name\u001b[38;5;241m=\u001b[39mOBJ_NAME, root_dir\u001b[38;5;241m=\u001b[39mP_PATH, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, img_size\u001b[38;5;241m=\u001b[39mimg_size, num_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/ml-workspace/Novel-View-Synthesis-with-NeRF/src/utils.py:266\u001b[0m, in \u001b[0;36mdata_preprocess\u001b[0;34m(obj_name, root_dir, img_size, min_max, num_points)\u001b[0m\n\u001b[1;32m    256\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[1;32m    258\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrays_o\u001b[39m\u001b[38;5;124m'\u001b[39m: rays_o_,\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrays_d\u001b[39m\u001b[38;5;124m'\u001b[39m: rays_d_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_img\u001b[39m\u001b[38;5;124m'\u001b[39m: current_img_\n\u001b[1;32m    265\u001b[0m }\n\u001b[0;32m--> 266\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_counter\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m file_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 618\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:466\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:424] . unexpected pos 85122368 vs 85122320"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from src.data_loader import *\n",
    "from src.utils import *\n",
    "from src.model import *\n",
    "\n",
    "SCALEDOWN = 2\n",
    "OBJ_NAME = 'chair'\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "img_size = int(800/SCALEDOWN)\n",
    "\n",
    "\n",
    "min_max = None\n",
    "\n",
    "data_preprocess(obj_name=OBJ_NAME, root_dir=P_PATH, img_size=img_size, num_points=32)\n",
    "\n",
    "#train dataset\n",
    "train_dataset = SynDatasetRay(obj_name=OBJ_NAME, root_dir=P_PATH, split=\"train\", img_size=img_size, num_points=8)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "min_max = train_dataset.min_max\n",
    "\n",
    "print(\"train dataset size: \", len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rays_o torch.Size([3])\n",
      "rays_d torch.Size([3])\n",
      "points torch.Size([8, 3])\n",
      "z_vals torch.Size([8, 1])\n",
      "v_dir torch.Size([2])\n",
      "rgb torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "#print the sample data shape\n",
    "sample = train_dataset[0]\n",
    "\n",
    "#sample is a dict with keys: rays_o, rays_d, points, z_vals, v_dir, img\n",
    "for key in sample:\n",
    "    print(key, sample[key].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 8, 60]) torch.Size([1024, 16])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "sample = next(iter(train_dataloader))\n",
    "\n",
    "# for key in sample:\n",
    "#     print(key, sample[key].shape)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "points = sample['points']\n",
    "v_dir = sample['v_dir']\n",
    "\n",
    "points_endcoded, v_dir_endcoded = position_encoding(points, v_dir, L_p=10, L_v=4)\n",
    "print(points_endcoded.shape, v_dir_endcoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model and volume rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rays_o torch.Size([1024, 3])\n",
      "rays_d torch.Size([1024, 3])\n",
      "points torch.Size([1024, 8, 3])\n",
      "z_vals torch.Size([1024, 8, 1])\n",
      "v_dir torch.Size([1024, 2])\n",
      "rgb torch.Size([1024, 3])\n",
      "\n",
      "torch.Size([1024, 8, 3]) torch.Size([1024, 8])\n",
      "\n",
      "torch.Size([1024, 3])\n",
      "torch.Size([1024, 3])\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#test model\n",
    "model = NeRF().to(device)\n",
    "model.eval()\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "sample = next(iter(train_dataloader))\n",
    "\n",
    "for key in sample:\n",
    "    print(key, sample[key].shape)\n",
    "print()\n",
    "\n",
    "points = sample['points'].to(device)\n",
    "v_dir = sample['v_dir'].to(device)\n",
    "z_vals = sample['z_vals'].to(device).squeeze()\n",
    "\n",
    "with torch.no_grad():\n",
    "    rgb, sigma = model(points, v_dir)\n",
    "    print(rgb.shape, sigma.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "#test volume rendering\n",
    "rendered_rgb = volume_rendering(z_vals, rgb, sigma)\n",
    "\n",
    "print(rendered_rgb.shape)\n",
    "print(sample['rgb'].shape)\n",
    "\n",
    "#calculate loss\n",
    "loss = torch.nn.functional.mse_loss(rendered_rgb, sample['rgb'].to(device))\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run a small training loop\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "model = NeRF(D=6, W=128, skips=[3]).to(device)\n",
    "model.train()\n",
    "BATCH_SIZE = 128\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "indice = torch.randint(0, 160000, (1000,))\n",
    "v_indice = torch.randint(0, 160000, (1000,))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=SubsetRandomSampler(indice), num_workers=NUM_WORKERS)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=SubsetRandomSampler(v_indice), num_workers=NUM_WORKERS)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     print(\"epoch: \", epoch)\n",
    "#     for i, sample in enumerate(train_dataloader):\n",
    "#         points = sample['points'].to(device)\n",
    "#         v_dir = sample['v_dir'].to(device)\n",
    "#         z_vals = sample['z_vals'].to(device).squeeze()\n",
    "#         rgb_gt = sample['rgb'].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         rgb, sigma = model(points, v_dir)\n",
    "#         rgb_pred = volume_rendering(z_vals, rgb, sigma)\n",
    "#         loss = torch.nn.functional.mse_loss(rgb_pred, rgb_gt)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         #validation loss\n",
    "#         if i % 10 == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 val_loss_total = 0\n",
    "#                 for j, sample in enumerate(val_dataloader):\n",
    "#                     points = sample['points'].to(device)\n",
    "#                     v_dir = sample['v_dir'].to(device)\n",
    "#                     z_vals = sample['z_vals'].to(device).squeeze()\n",
    "#                     rgb_gt = sample['rgb'].to(device)\n",
    "\n",
    "#                     rgb, sigma = model(points, v_dir)\n",
    "#                     print(\"rgb shape: \", rgb.shape)\n",
    "#                     print(\"sigma shape: \", sigma.shape)\n",
    "#                     print(\"z_vals shape: \", z_vals.shape)\n",
    "#                     rgb_pred = volume_rendering(z_vals, rgb, sigma)\n",
    "#                     loss = torch.nn.functional.mse_loss(rgb_pred, rgb_gt)\n",
    "#                     val_loss_total += loss.item()\n",
    "\n",
    "#                 print(\"val loss: \", val_loss_total / len(val_dataloader))\n",
    "#                 print(\"train loss: \", loss.item())\n",
    "                    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test trainer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeter90519051\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shao-yu-huang/Desktop/ml-workspace/Novel-View-Synthesis-with-NeRF/wandb/run-20231210_151512-4nuuk5j7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peter90519051/nerf/runs/4nuuk5j7' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/peter90519051/nerf' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peter90519051/nerf' target=\"_blank\">https://wandb.ai/peter90519051/nerf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peter90519051/nerf/runs/4nuuk5j7' target=\"_blank\">https://wandb.ai/peter90519051/nerf/runs/4nuuk5j7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from src.trainer import *\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "\n",
    "#init model\n",
    "D = 6\n",
    "W = 128\n",
    "input_ch_pos = 3\n",
    "input_ch_dir = 2\n",
    "L_p = 10\n",
    "L_v = 4\n",
    "skips = [3]\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "lr = 1e-3\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "wandb.init(project=\"nerf\", \n",
    "           name=\"test\",\n",
    "           config={\n",
    "                \"D\": D,\n",
    "                \"W\": W,\n",
    "                \"input_ch_pos\": input_ch_pos,\n",
    "                \"input_ch_dir\": input_ch_dir,\n",
    "                \"L_p\": L_p,\n",
    "                \"L_v\": L_v,\n",
    "                \"skips\": skips,\n",
    "                \"lr\": lr,\n",
    "                \"BATCH_SIZE\": BATCH_SIZE\n",
    "              }  \n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "model = NeRF(D=D, W=W, input_ch_pos=input_ch_pos, input_ch_dir=input_ch_dir, L_p=L_p, L_v=L_v, skips=skips).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "#init optimizer\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "#create a subset of the train dataset\n",
    "train_i = torch.randint(0, 160000*2, (10000,))\n",
    "val_i = torch.randint(0, 160000*2, (10000,))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,sampler=SubsetRandomSampler(train_i), batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "val_dataloader = DataLoader(train_dataset,sampler=SubsetRandomSampler(val_i), batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "#